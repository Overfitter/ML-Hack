{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Utilities\n",
    "import sys, os\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "#import en_core_sci_sm\n",
    "import string\n",
    "import unicodedata\n",
    "import spacy\n",
    "import nltk\n",
    "nltk.download('stopwords');\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize.toktok import ToktokTokenizer\n",
    "from flashtext import KeywordProcessor\n",
    "\n",
    "stopwords = set(set(stopwords.words('english')) -set([\"or\"]))\n",
    "\n",
    "# class\n",
    "class text_preprocessing:\n",
    "    def __init__(self):\n",
    "        #self.nlp = en_core_sci_sm.load()\n",
    "        self.nlp = spacy.load('en_core_sci_lg')\n",
    "        \n",
    "    def clean_sentence(self, text):\n",
    "        \"\"\"\n",
    "        Purpose: Function to Clean the text\n",
    "           1. Lowering the text\n",
    "           2. Removing Punctuations\n",
    "           3. Removing Numbers\n",
    "           4. Lemmatization of text\n",
    "           5. Removing Stopwords\n",
    "           6. Removing Whitespaces\n",
    "        Input: Raw string/text\n",
    "        Output: Clean string/text\n",
    "        \"\"\"\n",
    "        text = text.lower()\n",
    "        text = unicodedata.normalize(\"NFKD\", text)\n",
    "        text = self.clean_text(text)\n",
    "        text = self.remove_punct(text)\n",
    "#         text = self.clean_numbers(text)\n",
    "        text = self.lemma_text(text)\n",
    "#         print(text)\n",
    "        text = self.remove_stopwords(text, is_lower_case=True)\n",
    "#         print(text)\n",
    "        text = \" \".join(text.split())\n",
    "        text = text.replace(\"'\", \"\")\n",
    "        text = text.strip()\n",
    "        return(text) \n",
    "    \n",
    "    def clean_text(self, text):\n",
    "        \"\"\"Removing Punctuations from the text\"\"\"\n",
    "        text = str(text)\n",
    "        for punct in \"/-'\":\n",
    "            text = text.replace(punct, ' ')\n",
    "        for punct in '&':\n",
    "            text = text.replace(punct, ' ')\n",
    "        for punct in '?!.,\"#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~' + '“”’':\n",
    "            text = text.replace(punct, ' ')\n",
    "        return(text)\n",
    "    \n",
    "    def clean_numbers(self, text):\n",
    "        \"\"\"Removing Numbers (0-9) from text\"\"\"\n",
    "        if bool(re.search(r'\\d', text)):\n",
    "            text = re.sub('[0-9]', ' ', text)\n",
    "        return(text)\n",
    "\n",
    "    def remove_punct(self, text):\n",
    "        clean_text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        while '  ' in clean_text: clean_text = clean_text.replace('  ', ' ')\n",
    "        return(clean_text)\n",
    "    \n",
    "    def lemma_text(self, text):\n",
    "        \"\"\"Converting the word into the root word (Lemmatization) using Scispacy Module\"\"\"\n",
    "        s = [token.lemma_ for token in self.nlp(text) if token.lemma_ != '-PRON-']\n",
    "        output = ' '.join(s)\n",
    "        return(output)\n",
    "    \n",
    "    def remove_stopwords(self, text, is_lower_case=True):\n",
    "        \"\"\"Removing stopwords after tokenizing the text using ToktokTokenizer\"\"\"\n",
    "        stopword_list = list(set(list(stopwords) + ['may','also','across','among','beside','however','yet','within']))\n",
    "        tokenizer = ToktokTokenizer()\n",
    "        tokens = tokenizer.tokenize(text)\n",
    "        tokens = [token.strip() for token in tokens]\n",
    "        if is_lower_case:\n",
    "            filtered_tokens = [token for token in tokens if token not in stopword_list]\n",
    "        else:\n",
    "            filtered_tokens = [token for token in tokens if token.lower() not in stopword_list]\n",
    "        filtered_text = \" \".join(filtered_tokens)\n",
    "        return(filtered_text)\n",
    "    \n",
    "    \n",
    "class keyword_extract:\n",
    "    def __init__(self, keyword_list):\n",
    "        self.text_clean_device = text_preprocessing()\n",
    "        self.original_keyword_list = keyword_list\n",
    "        self.cleaned_keyword_list = [self.text_clean_device.clean_sentence(keyword) for keyword in self.original_keyword_list]\n",
    "        self._keyword_map_dic = dict(zip(self.cleaned_keyword_list, self.original_keyword_list))\n",
    "        self.keyword_processor = KeywordProcessor(case_sensitive=False)\n",
    "        self.keyword_processor.add_keywords_from_list(self.cleaned_keyword_list)\n",
    "        \n",
    "    def _exact_match(self, text):\n",
    "        reg_joinedpunct = re.compile(r'\\d+/[a-zA-Z]+', re.S)\n",
    "        sentence = text\n",
    "        sentence = sentence.replace(\"â\\x89¤\", \"less than equal to\")\n",
    "        sentence = sentence.replace(\"â\\x89¥\", \"greater than equal to\")\n",
    "        sentence = re.sub(\"\\n|\\r\", \"\", re.sub(\" +\", \" \", sentence.strip()))\n",
    "        sentence = re.sub('([.,;:!?{}()])', r' \\1 ', sentence)\n",
    "        sentence = re.sub('\\s{2,}', ' ', sentence)\n",
    "        sentence = reg_joinedpunct.sub(lambda m: m.group().replace(\"/\", \" \", 1), sentence)\n",
    "        sentence = re.sub(r\"(\\d+) , *?(\\d+)\", r\"\\1,\\2\", sentence)\n",
    "        sentence = re.sub(r\"(\\d+) \\. *?(\\d+)\", r\"\\1.\\2\", sentence)\n",
    "        sentence = re.sub(\"-\", \" \", sentence)\n",
    "        keywords_found_list = sorted(list(set([keyword[0] for keyword in self.keyword_processor.extract_keywords(sentence, span_info=True)])))\n",
    "        return(keywords_found_list)\n",
    "           \n",
    "    def extract_from(self, raw_text):\n",
    "        clean_text = self.text_clean_device.clean_sentence(raw_text)\n",
    "        keywords_found_list = self._exact_match(clean_text)\n",
    "        original_keywords_map_list = [self._keyword_map_dic[keyword] for keyword in keywords_found_list]\n",
    "        return(original_keywords_map_list)\n",
    " \n",
    "\n",
    "class basic_keyword_extract:\n",
    "    \"\"\"Basic keyword extractor class using the keyword processor by Flashtext\"\"\"\n",
    "    def __init__(self, keyword_list, case_sensitive=False):\n",
    "        self.keyword_list = keyword_list\n",
    "        self.case_sensitive = case_sensitive\n",
    "        self.keyword_processor = self._initialize_basic_keyword_processor()\n",
    "  \n",
    "    def _initialize_basic_keyword_processor(self):\n",
    "        kp = KeywordProcessor(case_sensitive=self.case_sensitive)\n",
    "        kp.add_keywords_from_list(self.keyword_list)\n",
    "        return kp\n",
    "  \n",
    "    def extract_from(self, raw_text):\n",
    "        if self.case_sensitive: \n",
    "            text = raw_text\n",
    "        else: \n",
    "            text = raw_text.lower()\n",
    "        clean_text = text.translate(str.maketrans(string.punctuation, ' '*len(string.punctuation)))\n",
    "        while '  ' in clean_text: clean_text = clean_text.replace('  ', ' ')\n",
    "        extracted_keywords_ls1 = self.keyword_processor.extract_keywords(text)\n",
    "        extracted_keywords_ls2 = self.keyword_processor.extract_keywords(clean_text)\n",
    "        return(extracted_keywords_ls1+extracted_keywords_ls2)\n",
    "\n",
    "\n",
    "def create_map_from_df(df, key_column, value_column):\n",
    "    df_temp = df[[key_column, value_column]].dropna()\n",
    "    df_temp = df_temp.apply(lambda x: x.astype(str).str.strip())\n",
    "    df_temp = df_temp.drop_duplicates().reset_index(drop=True)\n",
    "    map_dict = dict(zip(df_temp[key_column].str.lower(), df_temp[value_column]))\n",
    "    return(map_dict)\n",
    "\n",
    "\n",
    "def extract_key_column(df, desc_col_name, key_column_keyword_df, advance_keyword_extractor=False, default_key_column_raw=None, default_key_column_standard=None, default_key_column_group=None):\n",
    "    df_copy = df.copy()\n",
    "    # Column names\n",
    "    _col__key_column_measure_type = 'key_column_measure_type'\n",
    "    _col__standardized_key_column_name = 'standardized_key_column_name'\n",
    "    _col__raw_keywords = 'raw_keywords'\n",
    "    temp_key_column_keyword_df = key_column_keyword_df.copy()\n",
    "\n",
    "    # Map dict between raw keywords and standard form + standard and group\n",
    "    raw_to_standard_map_dict = create_map_from_df(temp_key_column_keyword_df, _col__raw_keywords, _col__standardized_key_column_name)\n",
    "\n",
    "    if default_key_column_raw and default_key_column_standard and default_key_column_group and (default_key_column_raw.lower() not in raw_to_standard_map_dict.keys()):\n",
    "        raw_to_standard_map_dict[default_key_column_raw] = default_key_column_standard\n",
    "#         raw_to_standard_map_dict[default_key_column_raw.lower()] = default_key_column_standard\n",
    "\n",
    "    raw_to_group_map_dict = create_map_from_df(temp_key_column_keyword_df, _col__raw_keywords, _col__key_column_measure_type)\n",
    "  \n",
    "    if default_key_column_raw and default_key_column_standard and default_key_column_group and (default_key_column_raw.lower() not in raw_to_group_map_dict.keys()):\n",
    "        raw_to_group_map_dict[default_key_column_raw] = default_key_column_group\n",
    "#         raw_to_group_map_dict[default_key_column_raw.lower()] = default_key_column_group\n",
    "\n",
    "    # List of unique keywords\n",
    "    key_column_keyword_ls = list(set(str(i).lower().strip() for i in temp_key_column_keyword_df[_col__raw_keywords].unique().tolist()))\n",
    "\n",
    "    # Intialize keyword processor\n",
    "    if advance_keyword_extractor:\n",
    "        end_extract_kp = keyword_extract(key_column_keyword_ls)\n",
    "    else:\n",
    "        end_extract_kp = basic_keyword_extract(key_column_keyword_ls, case_sensitive=False)\n",
    "    \n",
    "    # Extraction from data\n",
    "    df_copy['extracted_key_column'] = df_copy[desc_col_name].apply(lambda x: np.nan if(pd.isna(x)) else '|'.join(set(e.lower().strip() for e in end_extract_kp.extract_from(x) if(e.strip()!=''))))\n",
    "\n",
    "    df_copy['extracted_key_column'] = df_copy['extracted_key_column'].replace({'  ':np.nan,' ':np.nan,'':np.nan,'|':np.nan})\n",
    "    if default_key_column_raw and default_key_column_standard and default_key_column_group:\n",
    "        df_copy['extracted_key_column'] = df_copy['extracted_key_column'].fillna(default_key_column_raw)\n",
    "#         df_copy['extracted_key_column'] = df_copy['extracted_key_column'].fillna(default_key_column_raw.lower())\n",
    "    \n",
    "    \n",
    "    #extra parts added\n",
    "    def _get_standard_endp(text):\n",
    "        endp_list = str(text).strip().split(\"|\")\n",
    "        std_list = []\n",
    "        group_list = []\n",
    "        for endp in endp_list:\n",
    "            endp = endp.strip()\n",
    "            if endp in raw_to_standard_map_dict:\n",
    "                st_end = raw_to_standard_map_dict[endp]\n",
    "            else:\n",
    "                st_end = 'Not Available'\n",
    "                \n",
    "                \n",
    "            if endp in raw_to_group_map_dict:\n",
    "                grp = raw_to_group_map_dict[endp]\n",
    "            else:\n",
    "                grp = 'Not Available'\n",
    "                \n",
    "            std_list.append(st_end)\n",
    "            group_list.append(grp)\n",
    "            \n",
    "        temp_dict = {'st_endp':\"|\".join(std_list), 'group':'|'.join(group_list)}\n",
    "        return temp_dict\n",
    "        \n",
    "            \n",
    "    df_pivot_down = df_copy.copy()\n",
    "    df_pivot_down.reset_index(drop=True, inplace=True)\n",
    "#     df_pivot_down = df_pivot_down.explode('extracted_key_column')\n",
    "    df_pivot_down = df_pivot_down.assign(extracted_key_column=df_pivot_down['extracted_key_column'].str.split('|')).explode('extracted_key_column')\n",
    "    df_pivot_down['standardized_key_column'] = df_pivot_down['extracted_key_column'].apply(lambda x: _get_standard_endp(x)['st_endp'])\n",
    "    df_pivot_down['key_column_group'] = df_pivot_down['extracted_key_column'].apply(lambda x: _get_standard_endp(x)['group'])\n",
    "    \n",
    "    df_copy['standardized_key_column'] = df_copy['extracted_key_column'].apply(lambda x: _get_standard_endp(x)['st_endp'])\n",
    "    df_copy['key_column_group'] = df_copy['extracted_key_column'].apply(lambda x: _get_standard_endp(x)['group'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    return df_copy, df_pivot_down\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     # Roll Down at extracted scale\n",
    "#     df_roll_down = df_copy.assign(extracted_key_column=df_copy['extracted_key_column'].str.split('|')).explode('extracted_key_column')\n",
    "#     df_roll_down = df_roll_down.drop(columns=[desc_col_name])\n",
    "#     df_roll_down = df_roll_down.replace('  ', ' ').replace(' ', '').replace('', np.nan).fillna(np.nan)\n",
    "#     df_roll_down = df_roll_down.dropna(subset=['extracted_key_column'])\n",
    "#     df_roll_down = df_roll_down.drop_duplicates().reset_index(drop=True)\n",
    "#     df_roll_down['standardized_key_column'] = df_roll_down['extracted_key_column'].apply(lambda x: np.nan if(pd.isna(x)) else raw_to_standard_map_dict[x] if(x in raw_to_standard_map_dict.keys()) else np.nan)\n",
    "#     df_roll_down['key_column_group'] = df_roll_down['extracted_key_column'].apply(lambda x: np.nan if(pd.isna(x)) else raw_to_group_map_dict[x] if(x in raw_to_group_map_dict.keys()) else np.nan)\n",
    "#     return df_copy\n",
    "#     return(df_roll_down)\n",
    "\n",
    "\n",
    "# if __name__ == '__main__':\n",
    "#     key_column_description_df = \n",
    "#     standardized_key_column_df = \n",
    "\n",
    "#     key_column_rolldown_df = extract_key_column(df=key_column_description_df,\n",
    "#                                             desc_col_name='key_column_description_text',\n",
    "#                                             key_column_keyword_df=standardized_key_column_df,\n",
    "#                                             advance_keyword_extractor=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = pd.read_excel(\"RAW_key_column_STANDARDIZED_key_column_MAPPING_MASTER_DATABASE.xlsx\", skiprows=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = df_mapping[['key_column', 'Standardized key_column','key_column Group']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df_mapping[['Standardized key_column (SME Driven)', 'key_column Group']]\n",
    "\n",
    "df_temp['key_column'] = df_temp['Standardized key_column']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping = df_mapping.append(df_temp, sort=True, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_column = pd.read_excel(\"key_column for standardization.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_key_column.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapping.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardized_key_column_df = df_mapping.copy()\n",
    "standardized_key_column_df.rename({'key_column':'raw_keywords', 'Standardized key_column':'standardized_key_column_name', 'key_column Group':'key_column_measure_type'}, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_available = \"Not Available\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_endp_std, df_pivot_down =  extract_key_column(df=df_key_column,\n",
    "                                desc_col_name='key_column Name',\n",
    "                                key_column_keyword_df=standardized_key_column_df,\n",
    "                                advance_keyword_extractor=True,\n",
    "                                default_key_column_group=not_available,\n",
    "                                default_key_column_standard=not_available,\n",
    "                                default_key_column_raw=not_available)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_endp_std.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pivot_down.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_endp_std.to_excel(\"std_key_column_output.xlsx\", index=False)\n",
    "df_pivot_down.to_excel(\"std_key_column_output_pivot_down.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
